{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gdown\n",
    "# !gdown 1zDCi0nnxjP3so8wPwc5JGIj55lWimFw4\n",
    "\n",
    "# !gdown 1p4cBOvRvSsUYdRdkjo4CPFiBqHyjw_87"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import r2_score, root_mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import pickle as pk\n",
    "from FS.pso import jfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_value = 0\n",
    "\n",
    "# 1. Set the `PYTHONHASHSEED` environment variable at a fixed value\n",
    "import os\n",
    "\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed_value)\n",
    "\n",
    "# 3. Set the `numpy` pseudo-random generator at a fixed value\n",
    "np.random.seed(seed_value)\n",
    "np.random.default_rng(seed=seed_value)\n",
    "\n",
    "# 4. Set the `tensorflow` pseudo-random generator at a fixed value\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Remove limit on dataframe columns\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "# Input a series containing X and y to create a windowed dataset\n",
    "def create_multi_features_windowed_dataset(series, window_size, horizon, batch_size, shuffle_buffer):\n",
    "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
    "    ds = ds.window(window_size + horizon, shift=1, drop_remainder=True)\n",
    "    ds = ds.flat_map(lambda w: w.batch(window_size + horizon))\n",
    "    ds = ds.shuffle(shuffle_buffer_size)\n",
    "    ds = ds.map(lambda w: (w[:-horizon, :-1], w[-horizon:, -1]))\n",
    "    ds = ds.batch(batch_size).prefetch(1)\n",
    "    return ds\n",
    "\n",
    "# Input a series containing X only to get Kp for the next timestep\n",
    "def forecast_next_timestep(model, series, window_size, batch_size=64):\n",
    "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
    "    ds = ds.window(window_size, shift=1, drop_remainder=True)\n",
    "    ds = ds.flat_map(lambda w: w.batch(window_size))\n",
    "    ds = ds.batch(batch_size).prefetch(1)\n",
    "    forecast = model.predict(ds).squeeze()\n",
    "    return forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_table(\"./omni2_all_years.dat\", sep=\"\\s+\",header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"omni2_all_years.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add column names in csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"omni2_all_years.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns of IDs and other indices\n",
    "df.drop(\n",
    "    [\n",
    "        \"ID for IMF SC\",\n",
    "        \"ID for SW Plasma SC\",\n",
    "        \"DST Index\",\n",
    "        \"AE-index\",\n",
    "        \"Ap-index\",\n",
    "        \"f10.7_index\",\n",
    "        \"AL-index\",\n",
    "        \"AU-index\",\n",
    "    ],\n",
    "    inplace=True,\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert year, day, hour to datetime\n",
    "time = pd.to_datetime(df[\"Year\"] * 1000 + df[\"Day\"], format=\"%Y%j\")\n",
    "time = time + pd.to_timedelta(df.Hour, unit=\"h\")\n",
    "df.index = time\n",
    "\n",
    "# Reduce the number of kp values from 28 to 10\n",
    "df[\"Kp\"] = df[\"Kp*10\"].apply(lambda x: round(x / 10))\n",
    "\n",
    "# Drop the old columns\n",
    "df = df.drop([\"Year\", \"Day\", \"Hour\", \"Kp*10\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chosen time range\n",
    "df = df[(df.index.year > 1975) & (df.index.year < 2024)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacement = [\n",
    "    999,\n",
    "    999.9,\n",
    "    9999999.0,\n",
    "    9999.0,\n",
    "    99.99,\n",
    "    9.999,\n",
    "    999.99,\n",
    "    999999.99,\n",
    "    99999.99,\n",
    "    99.9,\n",
    "]\n",
    "# Replace missing values with NaN\n",
    "df.replace(replacement, np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolate the missing values\n",
    "df.interpolate(method=\"time\", limit_direction=\"both\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop some redundant features\n",
    "df.drop([\"By,GSM\", \"Bz,GSM\", \"Field Magnitude Avg\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pso_features = [\n",
    "#     \"Magnitude of Average Field vector\",\n",
    "#     \"Bx,GSE\",\n",
    "#     \"Sigma-B\",\n",
    "#     \"Sigma-Bx\",\n",
    "#     \"Sigma-By\",\n",
    "#     \"Sigma-Bz\",\n",
    "#     \"Na/Np\",\n",
    "#     \"Sigma-phi-V\",\n",
    "#     \"Sigma-theta-V\",\n",
    "#     \"PROT Flux  >1 MeV\",\n",
    "#     \"PROT Flux  >2 MeV\",\n",
    "#     \"PROT Flux  >30 MeV\",\n",
    "#     \"PROT Flux  >60 MeV\",\n",
    "#     \"PC(N)\",\n",
    "#     \"Kp\",\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature indices that were selected by PSO\n",
    "pso_features = [3, 6, 10, 11, 12, 13, 19, 24, 25, 31, 32, 35, 36, 38, 40]\n",
    "df = df.iloc[:, pso_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No splitting for production model\n",
    "train = df\n",
    "test = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = df.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = train[\"Kp\"]\n",
    "test_y = test[\"Kp\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data except for Kp\n",
    "scaler = StandardScaler()\n",
    "train[train.columns[:-1]] = scaler.fit_transform(train[train.columns[:-1]])\n",
    "test[test.columns[:-1]] = scaler.transform(test[test.columns[:-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicate the Kp column for the windowed dataset\n",
    "train_xy = pd.concat([train, train_y], axis=1)\n",
    "test_xy = pd.concat([test, test_y], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters for the dataset and model\n",
    "window_size = 24 # window size in hours\n",
    "horizon = 24 # forecast horizon in hours\n",
    "batch_size = 64\n",
    "shuffle_buffer_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create windowed datasets\n",
    "ds = create_multi_features_windowed_dataset(\n",
    "    train_xy, window_size, horizon, batch_size=batch_size, shuffle_buffer=shuffle_buffer_size\n",
    ")\n",
    "test_ds = create_multi_features_windowed_dataset(\n",
    "    test_xy, window_size, horizon, batch_size=batch_size, shuffle_buffer=shuffle_buffer_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.GRU(\n",
    "            100,\n",
    "            input_shape=(window_size, num_features),\n",
    "            return_sequences=True,\n",
    "        ),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.GRU(\n",
    "            100,\n",
    "            input_shape=(window_size, num_features),\n",
    "        ),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(\n",
    "            32,\n",
    "            activation=\"relu\",\n",
    "        ),\n",
    "        tf.keras.layers.Dense(horizon),\n",
    "    ]\n",
    ")\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "model.compile(loss=\"mse\", optimizer=optimizer)\n",
    "history = model.fit(ds, epochs=20, validation_data=test_ds, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loss\n",
    "plt.plot(history.history[\"loss\"], label=\"train\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"test\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model on test data\n",
    "pred = forecast_next_timestep(model, test, window_size)\n",
    "pred_discrete = pred.round()\n",
    "pred_discrete = np.clip(pred_discrete, 0, 9)\n",
    "# Remove the last prediction as it does not have a corresponding true value\n",
    "pred_discrete = pred_discrete[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cut the window size from the true values as it does not have a corresponding prediction\n",
    "real = test_y[window_size:]\n",
    "\n",
    "# Binary values to classify storms\n",
    "real_binary = (real < 5).astype(int)\n",
    "pred_binary = (pred_discrete < 5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results for 1 horizon value\n",
    "x = test_y.index[window_size:]\n",
    "true_interval = real\n",
    "pred_interval = pred_discrete[:,0]\n",
    "\n",
    "plt.plot(x, true_interval, label=\"True\", color=\"blue\")\n",
    "plt.plot(x, pred_interval, label=\"Prediction\", color=\"red\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "r2_scores = []\n",
    "f1_scores = []\n",
    "rmses = []\n",
    "rmses.append(root_mean_squared_error(real, pred_discrete[:, 0]))\n",
    "r2_scores.append(r2_score(real, pred_discrete[:, 0]))\n",
    "f1_scores.append(\n",
    "    classification_report(real_binary, pred_binary[:, 0], output_dict=True)[\"0\"][\n",
    "        \"f1-score\"\n",
    "    ]\n",
    ")\n",
    "# Metrics for multiple horizon values\n",
    "for h in range(1, horizon):\n",
    "    rmses.append(root_mean_squared_error(real[h:], pred_discrete[:-h, h]))\n",
    "    r2_scores.append(r2_score(real[h:], pred_discrete[:-h, h]))\n",
    "    f1_scores.append(\n",
    "        classification_report(real_binary[h:], pred_binary[:-h, h], output_dict=True)[\n",
    "            \"0\"\n",
    "        ][\"f1-score\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rmses)\n",
    "print(r2_scores)\n",
    "print(f1_scores)\n",
    "# Average metrics for all horizon values\n",
    "print(f\"RMSE: {np.mean(rmses)}\")\n",
    "print(f\"R2 Score: {np.mean(r2_scores)}\")\n",
    "print(f\"F1 Score: {np.mean(f1_scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(r2_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for 1 horizon value\n",
    "cm = confusion_matrix(real, pred_discrete[:, 0])\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "\n",
    "# Classification report\n",
    "\n",
    "print(classification_report(real, pred_discrete[:, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for binary classification\n",
    "cm = confusion_matrix(real_binary, pred_binary[:, 0])\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    xticklabels=[\"Storm\", \"Safe\"],\n",
    "    yticklabels=[\"Storm\", \"Safe\"],\n",
    ").set(xlabel=\"Predicted\", ylabel=\"Real\")\n",
    "\n",
    "print(classification_report(real_binary, pred_binary[:, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For future model usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"./exports/model.keras\")\n",
    "with open('./exports/standard_scaler.pkl', 'wb') as pickle_file:\n",
    "    pk.dump(scaler, pickle_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-25 15:24:09.057249: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-25 15:24:09.092531: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-25 15:24:09.092560: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-25 15:24:09.093535: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-25 15:24:09.098996: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-25 15:24:09.099377: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-25 15:24:09.708457: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/usr/lib/python3/dist-packages/requests/__init__.py:87: RequestsDependencyWarning: urllib3 (2.0.2) or chardet (4.0.0) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "from enum import Enum\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle as pk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feature(Enum):\n",
    "    MAG_AVG_FIELD = 0\n",
    "    BX_GSE = 1\n",
    "    SIGMA_B = 2\n",
    "    SIGMA_BX = 3\n",
    "    SIGMA_BY = 4\n",
    "    SIGMA_BZ = 5\n",
    "    NA_NP = 6\n",
    "    SIGMA_PHI_V = 7\n",
    "    SIGMA_THETA_V = 8\n",
    "    PROT_FLUX_1 = 11\n",
    "    PROT_FLUX_2 = 12\n",
    "    PROT_FLUX_30 = 13\n",
    "    PROT_FLUX_60 = 14\n",
    "    PC_N = 10\n",
    "    KP = 9\n",
    "\n",
    "# Order of features in the trained model\n",
    "features_order = [f.value for f in Feature]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the new downloaded data for the past 24hrs\n",
    "with open('./omni2_cl5f6i8QEF.lst', 'r',) as f:\n",
    "    data = []\n",
    "    for line in f:\n",
    "        data.append(line.split())\n",
    "df = pd.DataFrame(np.array(data, dtype=float))\n",
    "# Remove the date columns\n",
    "df.drop([0,1,2], axis='columns', inplace=True)\n",
    "# Rename the columns\n",
    "df.columns = range(df.shape[1])\n",
    "# Reorder the columns to match the model features order\n",
    "df = df[features_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and scaler\n",
    "scaler = pk.load(open('./exports/standard_scaler.pkl', 'rb'))\n",
    "model = tf.keras.models.load_model('./exports/model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # returns Kp(np.array), storm(bool)\n",
    "def forecast(scaler, model, data):\n",
    "    # Scale Kp values from Kp*10 to Kp\n",
    "    df[df.columns[-1]] = df[df.columns[-1]] / 10\n",
    "    # Scale the data except for kp\n",
    "    df[df.columns[:-1]] = scaler.transform(df[df.columns[:-1]])\n",
    "    # Reshape the data to match the model input\n",
    "    data = np.expand_dims(df, axis=0)\n",
    "    pred = model.predict(data).squeeze()\n",
    "    pred = pred.round()\n",
    "    pred = np.clip(pred, 0, 9)\n",
    "    # Check if any of the predictions are stormy\n",
    "    storm = (pred >= 5).any()\n",
    "    return (pred, storm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
